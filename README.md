# Ti-MAE: Self-Supervised Masked Time Series Autoencoders
This is an unofficial implementation of Ti-MAE in PyTorch Lightning. The original paper can be found [here](https://arxiv.org/abs/2301.08871).  

> [!NOTE]  
> This is a work in progress.

## Installation
* Python >= 3.9  
* PyTorch  
* PyTorch Lightning  

## TODO
- Add training script & results  
- Complete unit tests  
- Add CrossMAE to Ti-MAE  

## References
* [Transformers ViTMAE](https://github.com/huggingface/transformers/blob/main/src/transformers/models/vit_mae/modeling_vit_mae.py)  
* [Ti-MAE Paper](https://arxiv.org/abs/2301.08871)  
* [StatQuest Decoder Transformer from scratch](https://github.com/StatQuest/decoder_transformer_from_scratch)  